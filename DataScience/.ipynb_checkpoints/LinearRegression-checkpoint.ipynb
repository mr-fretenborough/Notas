{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"/datasets/SuperStoreOrders.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [**What is it?**](http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm)\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable and the other is considered to be a dependent variable. A common oversight with the implementation of a linear regressor is properly determining whether or not there is a level of causality involved in the relationship between the two variables. The purpose of finding this relationship is the ability to generate a prediction (or output value) based on a single feature. Obviously, the first thing to consider with linear regression is the equation for a line\n",
    "$$\n",
    "f(x)=mx+b\n",
    "$$\n",
    "where $m$ is the slope, $b$ is the y-intercept, $x$ is the feature, and $f(x)$ is the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction (m, x, b):\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Common Implementation**\n",
    "The most common implementation for creating a linear regressor is shifting a line such that the loss--or overall difference between the line and the actual values--has been minimized. This method is called least squares regression, and the loss function is as follows:\n",
    "$$\n",
    "Loss=\\sum_{i=0}^{n}(y_i-y_i^p)^2\n",
    "$$\n",
    "where $y_i$ represents the $i^{th}$ outcome and $y_i^p$ represents the $i^{th}$ prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squared_error(actual, prediction):\n",
    "    return sum([y + yhat for y, yhat in zip(actual, prediction)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad95fe08a8548265a01c14baa82179f83af9e6907fc61f7a9bea566e8167267"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
