{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "dataset = pd.read_csv(\"file:/Users/kier/OneDrive/Notas/DataScience/datasets/SuperStoreOrders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sales'] = (np.array([int(feature.replace(\",\", \"\")) for feature in dataset['sales']]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**What is it?**](http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm)\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable and the other is considered to be a dependent variable. A common oversight with the implementation of a linear regressor is properly determining whether or not there is a level of causality involved in the relationship between the two variables. The purpose of finding this relationship is the ability to generate a prediction (or output value) based on a single feature. Obviously, the first thing to consider with linear regression is the equation for a line\n",
    "$$\n",
    "f(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n + \\theta_{n+1}\n",
    "$$\n",
    "where $\\theta_i$ are the constants, $\\theta_{n+1}$ is the y-intercept, $x_i$ are the features, and $f(x)$ is the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction (theta, features):\n",
    "    if isinstance(features, pd.Series):\n",
    "        return [theta[0] * feature + theta[1] for feature in features]\n",
    "    return np.array([\n",
    "        sum([t * x for t, x in zip(theta, feature)]) + theta[-1] for feature in features\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Common Implementation**\n",
    "The most common implementation for creating a linear regressor is shifting a line such that the loss--or overall difference between the line and the actual values--has been minimized. This method is called least squares regression, and the loss function is as follows:\n",
    "$$\n",
    "Loss=\\sum_{i=0}^{n}(y_i-y_i^p)^2\n",
    "$$\n",
    "where $y_i$ represents the $i^{th}$ outcome and $y_i^p$ represents the $i^{th}$ prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squared_error(actual, prediction):\n",
    "    return sum([y + yhat for y, yhat in zip(actual, prediction)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Univariate vs Multivariate**]()\n",
    "As stated above, when it comes to linear regression, its important to choose features and observations such that you will likely find some relationship between them. This also includes cases where you have a _set_ of features as opposed to a single one, this is where the real linear algebra comes out. Given we cannot visualize higher dimensions, we have to rely on mathematics to determine whether our regressor is operating properly when we have more than two features. Furthermore, this means that we will have more values to tweak in effort to find the optimal line through the hyperspace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column  0: order_id        object\n",
      "Column  1: order_date      object\n",
      "Column  2: ship_date       object\n",
      "Column  3: ship_mode       object\n",
      "Column  4: customer_name   object\n",
      "Column  5: segment         object\n",
      "Column  6: state           object\n",
      "Column  7: country         object\n",
      "Column  8: market          object\n",
      "Column  9: region          object\n",
      "Column 10: product_id      object\n",
      "Column 11: category        object\n",
      "Column 12: sub_category    object\n",
      "Column 13: product_name    object\n",
      "Column 14: sales           int64\n",
      "Column 15: quantity        int64\n",
      "Column 16: discount        float64\n",
      "Column 17: profit          float64\n",
      "Column 18: shipping_cost   float64\n",
      "Column 19: order_priority  object\n",
      "Column 20: year            int64\n"
     ]
    }
   ],
   "source": [
    "for i, (column, type) in enumerate(zip(dataset.columns, dataset.dtypes)):\n",
    "    print(f\"Column {i:2}: {column:15} {type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = dataset['profit']\n",
    "features_univariate = dataset['sales']\n",
    "features_multivariate = dataset[['sales', 'quantity', 'discount', 'category', 'country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten univariate observations:\n",
      "0    408\n",
      "1    120\n",
      "2     66\n",
      "3     45\n",
      "4    114\n",
      "5     55\n",
      "6    314\n",
      "7    276\n",
      "8    912\n",
      "9    667\n",
      "Name: sales, dtype: int64\n",
      "The first 10 multivariate observations:\n",
      "   sales  quantity  discount         category      country\n",
      "0    408         2       0.0  Office Supplies      Algeria\n",
      "1    120         3       0.1  Office Supplies    Australia\n",
      "2     66         4       0.0  Office Supplies      Hungary\n",
      "3     45         3       0.5  Office Supplies       Sweden\n",
      "4    114         5       0.1        Furniture    Australia\n",
      "5     55         2       0.1  Office Supplies    Australia\n",
      "6    314         1       0.0       Technology       Canada\n",
      "7    276         1       0.1  Office Supplies    Australia\n",
      "8    912         4       0.4       Technology  New Zealand\n",
      "9    667         4       0.0        Furniture         Iraq\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first 10 univariate observations:\\n{features_univariate.head(10)}\")\n",
    "print(f\"The first 10 multivariate observations:\\n{features_multivariate.head(10)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**The Actual Regression**]()\n",
    "Now we get to actually do the regression! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Outliers and Influential Observations**](https://www.stat.cmu.edu/~larry/=stat401/lecture-20.pdf)\n",
    "An outlier is a point in the dataset which lies close to others in horizontal proximity, but not so much in vertical. An influential observation is similar to outliers in that they're a point which lies horizontally far away from the other data. These kinds of point are important because they can--without tending to--have a big impact on the final line generated through the linear regressor. Another way of looking at these kinds of points are \"outlier\\[s\\] are points with a large residual\" and influential points are those with a \"large impact on the regression\", noting that these two conditions are not mutually exclusive. \n",
    "\n",
    "### _Identifying Outliers_\n",
    "There are three ways we can decide if a point is an outlier:\n",
    "1. Look at the point's leverage\n",
    "2. Look at the point's studentized residuals\n",
    "3. Look at the point's Cook's statistics\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [_Leverage_]()\n",
    "Leverage is the measure of how far away the values of an observation are from those of neighboring observations along the axis which represents the independent variable. This can be described as:\n",
    "$$\n",
    "\\hat{Y}=HY\n",
    "$$\n",
    "where $H$ is the hat matrix. This means that each element of $Y$ (or $\\hat{Y_i}$) is a linear combination of elements of $H$. In particular, $H_{ii}$ is the contribution of the $i^{th}$ data point to $\\hat{Y_i}$. $\\therefore$ we call $h_{ii} \\equiv H_{ii}$ the leverage.\n",
    "\n",
    "Another expression for leverage, given by [jbstatistics](https://www.youtube.com/watch?v=xc_X9GFVuVU) is as follows:\n",
    "$$\n",
    "\\frac{1}{n} + \\frac{\\left(X_i - \\bar{X} \\right)^2}{SS_{XX}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [_The Hat Matrix_](https://en.wikipedia.org/wiki/Projection_matrix)\n",
    "The hat matrix (sometimes called the projection matrix or influence matrix) is a matrix which maps the vector of response values to the vector of fitted values. In other words, it maps the set of observations to the set of predictions. Furthermore, this matrix describes the influence each observation has on each prediction. The leverage values can be found on the diagonals of this matrix. You can use the hat matrix in the equation:\n",
    "$$\n",
    "\\hat{y} = \\bold{H} y\n",
    "$$\n",
    "where $\\hat{y}$ is the prediction, $y$ is the observation, and $\\bold{H}$ is the hat matrix. Moreover, the element at the $i^{th}$ row and $j^{th}$ column of $\\bold{H}$ is equal to the _covariance_ between the $j^{th}$ observation and the $i^{th}$ prediction divided by the variance of the former; depicted as:\n",
    "$$\n",
    "h_{ij} = \\frac{Cov\\left[\\hat{y_i}, y_j \\right]}{Var\\left[y_j \\right]}\n",
    "$$\n",
    "You may also recall Covariance and Variance being:\n",
    "$$\n",
    "Cov(X,Y) = E\\left[\\left(X - E[X] \\right) \\left(Y - E[Y] \\right) \\right]\n",
    "\\newline\n",
    "Var(X) = E \n",
    "\\left[ \n",
    "    \\left(\n",
    "        X - \\mu\n",
    "    \\right) ^ 2\n",
    "\\right]\n",
    "$$\n",
    "where $E[X]$ is the expected value of X (also called the mean of X). Please note that $Var(X)$ is sometimes denoted as $\\sigma_X^2$ and $Cov(X,Y)$ is sometimes denoted as $\\sigma_{XY}$. Also, $Var(X) = Cov(X, X)$\n",
    "\n",
    "##### [_Properties of the Hat Matrix_](https://www.sciencedirect.com/topics/mathematics/hat-matrix)\n",
    "The first condition is that for the elements along the diagonal of $\\bold{H}$, their values are $0 < \\bold{H_{ij}} < 1$ and $-1 < \\bold{H_{ij}} < 1$ for all others.\n",
    "\n",
    "For a model with an intercept term and the full rank of matrix, X: $\\sum_{i=1}^{n}\\bold{H_{ii}} = m$ and $\\sum_{i=1}^{n}\\bold{H_{ij}} = 1$. $\\therefore$ The mean value of the diagonal element $\\bold{H_{ii}}=\\frac{m}{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2 (default, Dec 21 2020, 15:06:04) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
